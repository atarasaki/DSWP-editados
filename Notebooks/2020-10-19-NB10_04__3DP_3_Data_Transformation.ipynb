{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NB10_04__3DP_3_Data_Transformation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atarasaki/DSWP-editados/blob/main/Notebooks/2020-10-19-NB10_04__3DP_3_Data_Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CgDLvphxfcX"
      },
      "source": [
        "<center><h1><b><i>3DP_3 - DATA TRANSFORMATION</i></b></h1></center>\n",
        "\n",
        "* **Objetivo**: Preparar os dados para o Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvW689ZBxbxH"
      },
      "source": [
        "# **AGENDA**:\n",
        "\n",
        "> Consulte **Table of contents**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNiuYCCxGe8v"
      },
      "source": [
        "# **Melhorias da sessão**\n",
        "* Desenvolver a sessão sobe WOE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TdSY74U0XS9"
      },
      "source": [
        "___\n",
        "# **Referências**\n",
        "* [Why, How and When to Scale your Features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n",
        "* [Demonstrating the different strategies of KBinsDiscretizer](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html#sphx-glr-auto-examples-preprocessing-plot-discretization-strategies-py);\n",
        "* [Why do we need feature scaling in Machine Learning and how to do it using SciKit Learn?](https://medium.com/@contactsunny/why-do-we-need-feature-scaling-in-machine-learning-and-how-to-do-it-using-scikit-learn-d8314206fe73)\n",
        "* [Importance of Feature Scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py) --> Muito importante por demonstrar os efeitos e a importância de se transformar as colunas numéricas.\n",
        "* [Feature discretization](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html#sphx-glr-auto-examples-preprocessing-plot-discretization-classification-py) --> Mostra o impacto na acurácia dos modelos com e sem discretização. Ou seja, discretizar faz sentido!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9DGifbWSmW3"
      },
      "source": [
        "___\n",
        "# **Machine Learning com Python (Scikit-Learn)**\n",
        "\n",
        "![Scikit-Learn](https://github.com/MathMachado/Materials/blob/master/scikit-learn-1.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg82Iouo_Qm2"
      },
      "source": [
        "# Porque dimensionar (Scale), padronizar (Standardize) e normalizar (Normalize) importa?\n",
        "* Porque muitos algoritmos de **Machine Learning** performam melhor ou convergem mais rápido quando os atributos/colunas/variáveis estão na mesma escala e possuem distribuição \"próxima\" da Normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-chlATnKSza"
      },
      "source": [
        "## Carregar as bibliotecas (genéricas) Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQGVQB18-tM_"
      },
      "source": [
        "!pip install category_encoders\n",
        "!pip install update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FJxrZckYxk6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import category_encoders as ce # library para aplicação do WOE - Weight Of Evidence para avaliar importância dos atributos\n",
        "\n",
        "# remove warnings to keep notebook clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyuWQM2NTMls"
      },
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0fuDyI8_UPf"
      },
      "source": [
        "## Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oRWtarakgMY"
      },
      "source": [
        "### Dataframe gerado aleatoriamente - variáveis com distribuição Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BXPXo3k0VDI"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "\n",
        "i_N = 10000\n",
        "\n",
        "df_A1 = pd.DataFrame({\n",
        "    'coluna1': np.random.normal(0, 2, i_N), # Observem que a média das colunas são distintas\n",
        "    'coluna2': np.random.normal(50, 3, i_N),\n",
        "    'coluna3': np.random.normal(-5, 5, i_N),\n",
        "    'coluna4': np.random.normal(-10, 10, i_N)\n",
        "})\n",
        "\n",
        "df_A1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ST1JnoRZKm"
      },
      "source": [
        "**Dica**: Podemos usar outras distribuições (se quisermos), como a Exponential (mostrada abaixo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUqjo5QcQH99"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "\n",
        "df_A2 = pd.DataFrame({\n",
        "    'coluna1': np.random.normal(0, 2, i_N),\n",
        "    'coluna2': np.random.normal(50, 3, i_N),\n",
        "    'coluna3': np.random.exponential(5, i_N), # coluna3 tem distribuição Exponential\n",
        "    'coluna4': np.random.normal(-10, 10, i_N)\n",
        "})\n",
        "\n",
        "df_A2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8MZNLbUkp8R"
      },
      "source": [
        "### Dataframe gerado aleatoriamente 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR-fDDujcTup"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "dados, classe = make_classification(n_samples = i_N, n_features = 4, n_informative = 3, n_redundant = 1, n_classes = 3)\n",
        "\n",
        "df_A3 = pd.DataFrame({'coluna1': dados[:,0],\n",
        "                                  'coluna2':dados[:,1],\n",
        "                                  'coluna3':dados[:,2],\n",
        "                                  'coluna4':dados[:,3]}) #, 'coluna5':classe})\n",
        "\n",
        "df_A3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq1cnpwLKvjS"
      },
      "source": [
        "df_A4 = pd.DataFrame({ \n",
        "    'coluna1': np.random.beta(5, 1, i_N) * 25, \n",
        "    'coluna2': np.random.exponential(5, i_N),\n",
        "    'coluna3': np.random.normal(10, 2, i_N),\n",
        "    'coluna4': np.random.normal(10, 10, i_N), \n",
        "})\n",
        "\n",
        "df_A4.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7sXQjvYRfhb"
      },
      "source": [
        "#### Extração de amostras para compararmos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjVHsnnHRkIo"
      },
      "source": [
        "df_A1_test = df_A1.sample(n = 100)\n",
        "df_A2_test = df_A2.sample(n = 100)\n",
        "df_A3_test = df_A3.sample(n = 100)\n",
        "df_A4_test = df_A4.sample(n = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0v0uXFRl-yG"
      },
      "source": [
        "___\n",
        "# **Transformações**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkzTO0fdz93b"
      },
      "source": [
        "## (1) StandardScaler\n",
        "* StandardScaler é a transformação que centraliza os dados através da remoção da média (dos dados) e, na sequência, redimensiona (scale) através da divisão pelo desvio-padrão;\n",
        "* Após a transformação, os dados terão média zero e desvio-padrão 1;\n",
        "* **Assume que os dados (as colunas a serem transformadas) são normalmente distribuidos**;\n",
        "* Se os dados não possuem distribuição Normal, então esta **NÃO** é uma boa transformação a se aplicar.\n",
        "\n",
        "$$z_{i}= \\frac{x_{i}-mean(x)}{std(x)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1UOOWeQ0R_Y"
      },
      "source": [
        "### Exemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1Lzx3xN6wpZ"
      },
      "source": [
        "df_A3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cPq_7Vu2HCS"
      },
      "source": [
        "Histograma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYW9WwBC3hd_"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "plt.hist(df_A1['coluna3'], color = 'blue', edgecolor = 'black', bins = int(180/5))\n",
        "\n",
        "# Adiciona títulos e labels\n",
        "plt.title('Coluna3 - Distribuição Normal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ogcQvvT5zK"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "plt.hist(df_A2['coluna3'], color = 'blue', edgecolor = 'black', bins = int(180/5))\n",
        "\n",
        "# Adiciona títulos e labels\n",
        "plt.title('Coluna3 - Distribuição Exponencial')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrgxkESc-Uaq"
      },
      "source": [
        "Considere o gráfico a seguir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7dHTF1W-Xsn"
      },
      "source": [
        "df_A1.plot(kind = 'kde') # KDE (= kernel Density Estimate) ajuda-nos a visualizar a distribuição dos dados, análogo ao histograma."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMS72n14-hDO"
      },
      "source": [
        "Qual a interpretação para o gráfico acima?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izqGNcNILdaX"
      },
      "source": [
        "df_A1.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEkAqlZg-p0v"
      },
      "source": [
        "A seguir, a transformação StandardScaler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4u3T_BX-oc_"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voFQ4odSzzPZ"
      },
      "source": [
        "O ideal é termos um array com as preditoras, da seguinte forma:\n",
        "X = [coluna1, coluna2, ..., colunaN]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPa4-SCt-ynX"
      },
      "source": [
        "np.set_printoptions(precision = 3)\n",
        "\n",
        "A1_scale = StandardScaler().fit_transform(df_A1) # Combinação dos métodos fit() + transform()\n",
        "\n",
        "A1_scale_fit = StandardScaler().fit(df_A1) # Aplica o fit() separadamente\n",
        "A1_scale_transform = A1_scale_fit.transform(df_A1) # Aplica o transform() separadamente.\n",
        "A1_scale_fit_transform = StandardScaler().fit(df_A1).transform(df_A1) # Aplica fit().transform() encadeado\n",
        "\n",
        "A2_scale = StandardScaler().fit_transform(df_A2)\n",
        "\n",
        "A3_scale = StandardScaler().fit_transform(df_A3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8tZJgbOplDd"
      },
      "source": [
        "## Salvar os parâmetros do StandardScaler e outros --> Colocar aqui!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERfRIz-njqcD"
      },
      "source": [
        "A1_scale_fit.scale_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioZ_IN3Z6d39"
      },
      "source": [
        "Observe abaixo que A1_scale = A1_scale_transform = A1_scale_fit_transform --> São arrays multidimensionais (do tipo NumPy)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xQR4cu5D1J"
      },
      "source": [
        "A1_scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6GtN2KF4E_A"
      },
      "source": [
        "A1_scale_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q2bvSqb6T4g"
      },
      "source": [
        "A1_scale_fit_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIhaErnA46Fi"
      },
      "source": [
        "Transformando em dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAhRvPze44JW"
      },
      "source": [
        "df_A1_scale = pd.DataFrame(A1_scale, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "df_A2_scale = pd.DataFrame(A2_scale, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "df_A3_scale = pd.DataFrame(A3_scale, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmQp8wDO_E88"
      },
      "source": [
        "Agora compare esse novo gráfico abaixo --> Vemos que os dados transformados tem distribuição Normal(0, 1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csfqRhDH2zUb"
      },
      "source": [
        "df_A1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-krh1pDg22RF"
      },
      "source": [
        "df_A1_scale.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2fTPWsm_Hq3"
      },
      "source": [
        "df_A1_scale.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oN-829l3277"
      },
      "source": [
        "df_A2.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqh8L5BeUHT-"
      },
      "source": [
        "df_A2_scale.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvz6O1zk4XNE"
      },
      "source": [
        "df_A3.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffU-fQxCUSmm"
      },
      "source": [
        "df_A3_scale.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24MOLL83w9j"
      },
      "source": [
        "### Exercício: Calcular a média e o desvio-padrão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Aa25gVlSdOi"
      },
      "source": [
        "df_A1.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXZUiZImSmOE"
      },
      "source": [
        "df_A1_scale.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIUQw5dpRwvA"
      },
      "source": [
        "#### Correlação das colunas\n",
        "* Observe que as correlações entre as variáveis não se alteram com as transformações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj1UerjORq9q"
      },
      "source": [
        "df_A1.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp6vPK0aR_p0"
      },
      "source": [
        "df_A1_scale.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fuURrao_M0c"
      },
      "source": [
        "Qual a conclusão?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0A9U7rs_RAT"
      },
      "source": [
        "## (2) MinMaxScaler\n",
        "* **Transformação muito popular e utilizada**.\n",
        "* Transforma os dados para o intervalo [0, 1];\n",
        "* Se StandardScaler não é aplicável, então essa transformação funciona bem.\n",
        "* Sensível aos _outliers_. Portanto, o ideal é que os _outliers_ sejam tratados previamente.\n",
        "* Uma transformação similar à MinMaxScaler() é MaxAbsScaler() (redimensiona os dados no intervalo [-1, 1]) e centralizado em 0).\n",
        "* Não corrige skewness;\n",
        "* Sensível à outliers;\n",
        "\n",
        "$$z_{i}= \\frac{x_{i}-min(x)}{max(x)-min(x)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0HbeuP-AU_p"
      },
      "source": [
        "### Exemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgeLckzxAWaC"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_W9bTO2AbEg"
      },
      "source": [
        "df_A1.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJRFbUpBAg5J"
      },
      "source": [
        "A1_MinMaxScaler = MinMaxScaler().fit_transform(df_A1)\n",
        "df_A1_MinMaxScaler = pd.DataFrame(A1_MinMaxScaler,columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "\n",
        "# Gráfico\n",
        "df_A1_MinMaxScaler.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g8GA4LTA40U"
      },
      "source": [
        "Qual a conclusão?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z6D3vfnB9Nm"
      },
      "source": [
        "## (3) RobustScaler\n",
        "* Transformação ideal para dados com **outliers**.\n",
        "\n",
        "$$z_{i}= \\frac{x_{i}-Q_{1}(x)}{Q_{3}(x)-Q_{1}(x)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3oyuxLeCW1D"
      },
      "source": [
        "df_A1.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeDF7-w_CcBy"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLoqSKijCf2v"
      },
      "source": [
        "A1_RobustScaler = RobustScaler().fit_transform(df_A1)\n",
        "df_A1_RobustScaler = pd.DataFrame(A1_RobustScaler, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "\n",
        "# Gráfico\n",
        "df_A1_RobustScaler.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_D-7ik2xXpU"
      },
      "source": [
        "### **Insight**: Gerar aleatoriamente colunas/variáveis com distribuição Gamma, Beta, Normal, Exponential e etc e avaliar o impacto das várias transformações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxIOPmSYwX-e"
      },
      "source": [
        "# **Wrap Up**\n",
        "* Use MinMaxScaler como transformação default, pois esta transformação não distorce os dados;\n",
        "* Use RobustScaler se seus dados/coluna/variável possui **outliers** e gostaríamos de reduzir o efeito/impacto destes **outliers**. Entretanto, o melhor tratamento é estudar os **outliers** cuidadosamente e tratá-los adequadamente;\n",
        "* Use StandardScaler se seus dados/colunas/variáveis possuem distribuição Normal (ou pelo menos se aproxima bem da distribuição Normal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YVMgt-WEFif"
      },
      "source": [
        "## Encoding Variáveis Categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHYvLc8T_jxQ"
      },
      "source": [
        "### Encoding Variáveis Ordinais\n",
        "* Exemplo: Variáveis com valores ordinais: baixo, médio ou alto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1BgGiGdSTcG"
      },
      "source": [
        "#### Dataframe-exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdVahfJAEkuO"
      },
      "source": [
        "# Aqui vou usar a função randint - Retorna números inteiros aleatórios incluindo o número inferior e excluindo o superior.\n",
        "\n",
        "l_idade = [\n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40), \n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40),\n",
        "           np.random.randint(20, 40)\n",
        "          ]\n",
        "\n",
        "l_salario = ['baixo', 'medio', 'alto']\n",
        "l_salario2 = np.random.choice(l_salario, 10, p = [0.6, 0.3, 0.1])\n",
        "\n",
        "df_A5 = pd.DataFrame({\n",
        "    'idade': l_idade,\n",
        "    'salario': l_salario2})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_15P2eUHSBY"
      },
      "source": [
        "df_A5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1g9pEuyHe2q"
      },
      "source": [
        "Neste exemplo, vamos redefinir a variável categórical ordinal 'Salario' da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkwFuEa8HnMV"
      },
      "source": [
        "df_A5['salario_cat'] = df_A5['salario'].map({'baixo': 1, 'medio': 2, 'alto': 3})\n",
        "df_A5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlaIFiWIIPAl"
      },
      "source": [
        "### Encoding Variáveis Nominais\n",
        "* Exemplo: Variáveis com valores nominais: Sexo (Feminino, Masculino).\n",
        "\n",
        "* Use One-Hot Encoding ou pd.get.dummies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffNoJQbgJRoY"
      },
      "source": [
        "Vamos utilizar o dataframe criado no passo anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMCoUWZOI7c0"
      },
      "source": [
        "df_A5['salario'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdIEyBkaJeN8"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MwK4cUEKeK4"
      },
      "source": [
        "#### Aplicar LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X6VXDsHJiII"
      },
      "source": [
        "le = LabelEncoder()\n",
        "df_A5['salario_le'] = le.fit_transform(df_A5['salario'])\n",
        "df_A5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY80x59J8Ham"
      },
      "source": [
        "df_A5['salario'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgv2Zz07Kqfj"
      },
      "source": [
        "#### Aplicar pd.get.dummies()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSZRIEs6K5sP"
      },
      "source": [
        "dummies = pd.get_dummies(df_A5['salario'])\n",
        "df_A5 = pd.concat([df_A5, dummies], axis = 1)\n",
        "df_A5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC37EB10LgR0"
      },
      "source": [
        "* 19/10/2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXKQ3lkg8qO"
      },
      "source": [
        "# Power Transformations\n",
        "* Tem por objetivo transformar a distribuição de probabilidade da variável/coluna a fim de torná-la Normal. Esta normalização é feita através da correção da skewness (estabilização da variância) da distribuição.\n",
        "* Exemplos de Power Transformations:\n",
        "    * log;\n",
        "        * Ajuda com distribuições skewness;\n",
        "        * Útil para distribuições não-negativas e sem zeros;\n",
        "    * raiz quadrada;\n",
        "    * raiz cúbica;\n",
        "    * **Transformação de Box-Cox** e\n",
        "    * Transformação de Yeo-Johson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQDf9EfzRXYC"
      },
      "source": [
        "### Transformação de Yeo-Johnson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9q9lxbYhlKE"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "plt.hist(df_A2['coluna3'], color = 'blue', edgecolor = 'black', bins = int(180/5))\n",
        "\n",
        "# Adiciona títulos e labels\n",
        "plt.title('Histograma da coluna3 - Distribuição Exponencial')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ91Stekh8JO"
      },
      "source": [
        "from sklearn.preprocessing import PowerTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCGFIeszkLVK"
      },
      "source": [
        "df_A2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snNcFWwGMnQr"
      },
      "source": [
        "1. dados = objeto.transform(dataframe)\n",
        "2. dados_transformados = fit(df).transform(df)\n",
        "3. dados_transformados = fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOQYdkyzi-PL"
      },
      "source": [
        "# cria objeto da transformação power\n",
        "yeo_johnson = PowerTransformer(method = 'yeo-johnson', standardize = True)\n",
        "# aplicação da transformação, obtendo array numpy\n",
        "A2_yeo_johnson = yeo_johnson.fit_transform(df_A2)\n",
        "# transformação do array numpy em dataframe pandas\n",
        "df_A2_yeo_johnson = pd.DataFrame(A2_yeo_johnson, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "df_A2_yeo_johnson.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_5beALqkao_"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "plt.hist(df_A2_yeo_johnson['coluna3'], color = 'blue', edgecolor = 'black', bins = int(180/5))\n",
        "\n",
        "# Adiciona títulos e labels\n",
        "plt.title('Coluna3 - Distribuição aproximadamente Normal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwEhZBARk7oA"
      },
      "source": [
        "## Transformação de Box-Cox\n",
        "* Inventada por dois grandes personagens da Estatística;\n",
        "* A coluna/variável/atributo não pode conter números negativos ou zero. Ou seja, $X_{i} > 0$.\n",
        "\n",
        "* Se $w_{i}$ é a variável transformada e $x_{i}$ é a variável que queremos transformar.\n",
        "    * Se $\\lambda = 0$ --> $w_{i}^{(\\lambda)} = \\log(x_{i})$;\n",
        "    * Se $\\lambda <> 0$ --> $w_{i}^{(\\lambda)} = \\frac{x_{i}-1}{\\lambda}$;\n",
        "* Se $\\lambda = 1$, então $w_{i}$ então os dados/distribuição já são normalmente distribuídos e a transformação de Box&Cox não se faz necessário.\n",
        "* Precisamos escolher o valor de $\\lambda$ que permite a melhor aproximação da distribuição normal.\n",
        "* A função **scipy.stats.boxcox(array_1D)** retorna o valor de **$\\lambda$ ótimo**. Basta passar como parâmetro o array de dimensão 1D que a função retorna o $\\lambda$ ótimo que melhor se ajusta aos seus dados.\n",
        "* Para retornar seus dados aos valores originais, use **scipy.special.inv_boxcox(y, lambda)**.\n",
        "* Quais são as desvantagens da transformação?\n",
        "    * **Perda da interpretação dos dados**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZXkW4Baz87T"
      },
      "source": [
        "Libraries necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzWRS8chz8V_"
      },
      "source": [
        "import numpy as np \n",
        "from scipy import stats \n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXUasfUXO7vW"
      },
      "source": [
        "# Gerando dados com distribuição Exponencial\n",
        "distribuicao_exponencial = np.random.exponential(size = 1000) \n",
        "\n",
        "# Dados transformados \n",
        "box_cox, lambda_box_cox = stats.boxcox(distribuicao_exponencial) \n",
        "f\"lambda ótimo : {lambda_box_cox}\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lwyGVDMzC4y"
      },
      "source": [
        "### Exemplo 1\n",
        "* Dados possuem distribuição Exponencial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx5J1L8Az4qQ"
      },
      "source": [
        "# Gráficos: \n",
        "def compara_graficos(y, w, lambda_box_cox):\n",
        "    fig, ax = plt.subplots(1, 2) \n",
        "  \n",
        "    # Gráfico das distribuições originais e transformada\n",
        "    sns.distplot(y, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Non-Normal\", color =\"green\", ax = ax[0]) \n",
        "    sns.distplot(w, hist = False, kde = True, kde_kws = {'shade': True, 'linewidth': 2}, label = \"Normal\", color =\"green\", ax = ax[1]) \n",
        "  \n",
        "    # Legendas \n",
        "    plt.legend(loc = \"upper right\") \n",
        "  \n",
        "    # Redimensionando os sub-gráficos \n",
        "    fig.set_figheight(5) \n",
        "    fig.set_figwidth(10) \n",
        "    \n",
        "    print(f\"Valor de Lambda usado na transformação: {lambda_box_cox}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf5PPVl9Rr5H"
      },
      "source": [
        "Transforma os dados/distribuições:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWLsXEBB0CQO"
      },
      "source": [
        "compara_graficos(distribuicao_exponencial, box_cox, lambda_box_cox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9jObBLCZh19"
      },
      "source": [
        "### Exemplo 2\n",
        "* Dados possuem distribuição Beta.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CePoB8imzPRQ"
      },
      "source": [
        "# Gerando dados com distribuição Exponencial\n",
        "distribuicao_beta = np.random.beta(1, 3, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1CwLPm6zRx2"
      },
      "source": [
        "# transform training data & save lambda value \n",
        "box_cox, lambda_box_cox = stats.boxcox(distribuicao_beta) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8feHp3rWN5p"
      },
      "source": [
        "f\"Lambda ótimo : {lambda_box_cox}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snd63l9U0ugI"
      },
      "source": [
        "compara_graficos(distribuicao_beta, box_cox, lambda_box_cox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ6o9pOkPjUT"
      },
      "source": [
        "### Transformação log\n",
        "* De forma geral, a transformação **log** trata de dados skewed (diferentes da distribuição Normal), tornando os dados (ou a distribuição dos dados) mais \"normal\";\n",
        "* Se os dados forem de alguma forma normalmente distribuídos, então nada muda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrsXETsRPupd"
      },
      "source": [
        "# Gerando dados com distribuição Exponencial\n",
        "distribuicao_beta = np.random.beta(1, 3, 1000)\n",
        "\n",
        "transformacao_log = np.log(distribuicao_beta)\n",
        "compara_graficos(distribuicao_beta, transformacao_log, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwh0alhdgrE3"
      },
      "source": [
        "___\n",
        "# **Exercícios**\n",
        "> Para cada um dos dataframes a seguir, aplique os seguintes steps:\n",
        "\n",
        "* Padronizar o nome das colunas\n",
        "    * Eliminar espaços entre os nomes das colunas;\n",
        "    * Eliminar caracteres especiais dos nomes das colunas;\n",
        "    * Renomear as colunas com lower() (ou upper());\n",
        "* Aplicar a trasformação StandardScaler e MinMaxScaler em cada uma das colunas do dataframe;\n",
        "* DataViz - Mostrar a distribuição das colunas para compararmos os resultados antes e depois das transformações.\n",
        "* As correlações das colunas mudam com as transformações?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSTKrd992LtI"
      },
      "source": [
        "## Exercício 1 - Iris --> **Resolvido**\n",
        "* [Aqui](https://en.wikipedia.org/wiki/Iris_flower_data_set) você obterá mais informações sobre o dataframe iris. Confira."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mThqvGGr2Vuk"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X= iris['data']\n",
        "y= iris['target']\n",
        "\n",
        "df_iris = pd.DataFrame(np.c_[X, y], columns= np.append(iris['feature_names'], ['target']))\n",
        "df_iris['target2'] = df_iris['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "df_iris.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU5FaJhdYblP"
      },
      "source": [
        "df_iris.columns = [c.replace(' ', '_') for c in df_iris.columns]\n",
        "df_iris.columns = [c.replace('_(cm)', '') for c in df_iris.columns]\n",
        "df_iris.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9DPAakJZQHH"
      },
      "source": [
        "df_iris.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYYmVq68Y8bB"
      },
      "source": [
        "# Aplica a transformação:\n",
        "df_iris_MinMaxScaler = MinMaxScaler().fit_transform(df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
        "\n",
        "# Transformando em Dataframe:\n",
        "df_iris_MinMaxScaler = pd.DataFrame(df_iris_MinMaxScaler, columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
        "\n",
        "# Gráfico\n",
        "df_iris_MinMaxScaler.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKsHcjd77YZT"
      },
      "source": [
        "### Aplicar as outras transformações e comparar os gráficos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caFkC6oCmUKK"
      },
      "source": [
        "## Exercício 2 - Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhOM-Z9zmf-f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X= cancer['data']\n",
        "y= cancer['target']\n",
        "\n",
        "df_A1_cancer = pd.DataFrame(np.c_[X, y], columns= np.append(cancer['feature_names'], ['target']))\n",
        "df_A1_cancer['target'] = df_A1_cancer['target'].map({0: 'malign', 1: 'benign'})\n",
        "df_A1_cancer.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qruqUDqnvMc"
      },
      "source": [
        "## Exercício 3 - Boston Housing Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trxK8YXNnsam"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "X= boston['data']\n",
        "y= boston['target']\n",
        "\n",
        "df_A1_boston = pd.DataFrame(np.c_[X, y], columns= np.append(boston['feature_names'], ['target']))\n",
        "df_A1_boston.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzu0Dz33c8ds"
      },
      "source": [
        "## Exercícios 4 - Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ahBZmqc_-1"
      },
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "X= diabetes['data']\n",
        "y= diabetes['target']\n",
        "\n",
        "df_A1_diabetes = pd.DataFrame(np.c_[X, y], columns= np.append(diabetes['feature_names'], ['target']))\n",
        "df_A1_diabetes.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyunIr6oaWEl"
      },
      "source": [
        "## Exercícios 5 - 120 years of Olympic history: athletes and results\n",
        "* [120 years of Olympic history: athletes and results](https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results)\n",
        "    * Trate adequadamente as variáveis 'sex', 'season', 'team', 'city', 'sport' e 'medal';\n",
        "    * Aplique as transformações que acabamos de estudar nos campos/colunas numéricas 'height' e 'weight'. Cuidado com os Missing Values contidos nas variáveis!\n",
        "    * Verifique/avalie o impacto dos outliers nestas colunas.\n",
        "    * Neste caso, qual transformação é mais adequado diante dos outliers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V7OCd3G9zj1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3g4dqM190mj"
      },
      "source": [
        "url = '/content/drive/My Drive/Datasets4ML/athlete_events.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ExKsjGmKaEx"
      },
      "source": [
        "## Exercício 6 - FIFA\n",
        "* Aplique as transformações MinMaxScaler, RobustScaler e StandardScaler às colunas numéricas do dataframe FIFA_algumas_features.csv.\n",
        "* Para as colunas categóricas, aplique a transformação mais adequada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjukr52HK3S_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(f'Versão do Pandas: {pd.__version__}')\n",
        "print(f'Versão do NumPy.: {np.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKuTnd5MariI"
      },
      "source": [
        "d_configuracao = {\n",
        "    'display.max_columns': 1000,\n",
        "    'display.expand_frame_repr': True,\n",
        "    'display.max_rows': 10,\n",
        "    'display.precision': 2,\n",
        "    'display.show_dimensions': True,\n",
        "    'display.float_format': (lambda x: '%.2f' % x)\n",
        "                  }\n",
        "\n",
        "for op, value in d_configuracao.items():\n",
        "    pd.set_option(op, value)\n",
        "    print(op, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S41tXs2EKlHN"
      },
      "source": [
        "#url = 'https://raw.githubusercontent.com/MathMachado/DataFrames/master/FIFA_algumas_features.csv?token=AGDJQ62CSW5KBLZNXH4TULK7SXICE'\n",
        "\n",
        "#df = pd.read_csv(url, index_col = 'ID')\n",
        "#df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afFcXMeAaiaM"
      },
      "source": [
        "df_fifa = pd.read_csv('https://raw.githubusercontent.com/atarasaki/DSWP/master/Dataframes/FIFA.csv',index_col = 'ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhIQpk3La1cu"
      },
      "source": [
        "df_fifa.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlmPafrOa-7z"
      },
      "source": [
        "df_fifa_org = df_fifa.copy\n",
        "df_fifa.drop(inplace=True, columns=['Photo','Flag','Club Logo','Real Face','Jersey Number','LS','ST','RS','LW','LF','CF','RF','RW','LAM','CAM','RAM','LM','LCM','CM','RCM','RM','LWB','LDM','CDM','RDM','RWB','LB','LCB','CB','RCB','RB'])\n",
        "df_fifa.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CnXy_hebixf"
      },
      "source": [
        "# expressões regulares : 'new_wage' recebe parte numérica de 'Wage'\n",
        "df_fifa['wage_val'] = df_fifa['Wage'].str.extract( '([\\d]+)' ).astype('float64')\n",
        "df_fifa.head()\n",
        "\n",
        "# função para associar potência de dez aos caracteres das colunas 'Value', 'Wage', 'Release Clause'\n",
        "func_multiplicador = lambda x: 1000.0 if x == 'K' else ( 1000000.0 if x == 'M' else 1.0 )\n",
        "\n",
        "df_fifa['wage_sufix'] = df_fifa['Wage'].str.extract( '[\\d]+(\\w)' ).astype('str')\n",
        "df_fifa.head()\n",
        "\n",
        "df_fifa['new_wage'] = df_fifa['wage_val'] * df_fifa['wage_sufix'].map( func_multiplicador )\n",
        "df_fifa.drop(inplace=True, columns=['wage_val','wage_sufix'])\n",
        "df_fifa.head()\n",
        "\n",
        "# campo 'Value'\n",
        "df_fifa['value_value'] = df_fifa['Value'].str.extract( '([\\d]+[\\.]*[\\d]+)' ).astype('float64')\n",
        "df_fifa['value_sufix'] = df_fifa['Value'].str.extract( '[\\d]+[\\.]*[\\d]+(\\w)' ).astype('str')\n",
        "df_fifa['new_value'] = df_fifa['value_value'] * df_fifa['value_sufix'].map( func_multiplicador ).head()\n",
        "df_fifa.drop(inplace=True, columns=['value_value','value_sufix'])\n",
        "\n",
        "df_fifa.head()\n",
        "\n",
        "# campo 'Release Clause'\n",
        "df_fifa['release_value'] = df_fifa['Release Clause'].str.extract( '([\\d]+[\\.]*[\\d]+)' ).astype('float64')\n",
        "df_fifa['release_sufix'] = df_fifa['Release Clause'].str.extract( '[\\d]+[\\.]*[\\d]+(\\w)' ).astype('str')\n",
        "df_fifa['new_release_value'] = df_fifa['release_value'] * df_fifa['release_sufix'].map( func_multiplicador ).head()\n",
        "df_fifa.drop(inplace=True, columns=['release_value','release_sufix'])\n",
        "\n",
        "df_fifa.head()\n",
        "\n",
        "# elimina as colunas originais\n",
        "df_fifa.drop(inplace=True, columns=['Wage','Value','Release Clause'])\n",
        "df_fifa.head()\n",
        "\n",
        "# passo 1 : extração do nome das colunas\n",
        "s_colunas = df_fifa.columns\n",
        "s_colunas\n",
        "\n",
        "# passo 2 : transformar o nome das colunas em minúsculas\n",
        "s_colunas.str.lower()\n",
        "\n",
        "# passo 3 : aplicar o nome das colunas em minúsculas\n",
        "df_fifa.columns = s_colunas.str.lower()\n",
        "df_fifa.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR5M43QCbpzu"
      },
      "source": [
        "# gera nome para a coluna 0\n",
        "df_fifa.rename( columns={'unnamed: 0':'seq'}, inplace=True )\n",
        "df_fifa.head()\n",
        "\n",
        "# mostra os valores das colunas\n",
        "d_col_val = {}\n",
        "for col in df_fifa.columns:\n",
        "  d_col_val[col] = df_fifa[col].unique()\n",
        "\n",
        "d_col_val\n",
        "\n",
        "# list comprehension para mostrar as colunas com missing values e as quantidades correspondentes\n",
        "l_missing_values_col = [ col for col in df_fifa.columns if df_fifa[col].isnull().sum() > 0 ]\n",
        "l_missing_values_col\n",
        "\n",
        "df_fifa.dtypes.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG6y-rgUbx03"
      },
      "source": [
        "# variáveis numéricas\n",
        "\n",
        "# aplicação da mediana para variáveis numéricas\n",
        "for col in df_fifa.columns:\n",
        "  if ( df_fifa[col].dtype == 'int64' ) or ( df_fifa[col].dtype == 'float64' ):\n",
        "    df_fifa[col].fillna( df_fifa[col].median(), inplace=True )\n",
        "\n",
        "df_fifa.isnull().sum()\n",
        "\n",
        "# avaliação da representatividade dos missing values\n",
        "ds_lin, ds_col = df_fifa.shape\n",
        "for col in df_fifa.columns:\n",
        "  soma_mv = df_fifa[col].isnull().sum()\n",
        "  if soma_mv > 0:\n",
        "    print(col, ': tipo ', df_fifa[col].dtype, ': qt ', soma_mv, ' : ', (soma_mv/ds_lin)*100, ' %' )\n",
        "\n",
        "# transformação de variáveis numéricas formatadas - height\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUth5aSEe8ND"
      },
      "source": [
        "# conversão de altura : feet'inch -> metros\n",
        "def func_converte_altura( altura_ingles ):\n",
        "  '''\n",
        "  conversão de altura : feet'inch -> metros\n",
        "  '''\n",
        "  altura_aux = altura_ingles.split( \"'\", 1 )\n",
        "  altura_feet = altura_aux[0]\n",
        "  try:\n",
        "    altura_inch = altura_aux[1]\n",
        "  except:\n",
        "    altura_inch = 0.0\n",
        "  return ( ( float( altura_feet ) * 12 ) + float( altura_inch ) ) * 0.0254\n",
        "\n",
        "\n",
        "df_fifa['new_height'] = df_fifa['height'].astype('str').map( func_converte_altura )\n",
        "df_fifa.head()\n",
        "\n",
        "\n",
        "df_fifa['new_height'].dtype\n",
        "\n",
        "df_fifa['new_height'].fillna( df_fifa['new_height'].median(), inplace=True )\n",
        "df_fifa.head()\n",
        "\n",
        "df_fifa['new_height'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxDSVVf_fTVH"
      },
      "source": [
        "df_fifa.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZwdTrEJfK8J"
      },
      "source": [
        "# transformação de variáveis numéricas formatadas - weight\n",
        "\n",
        "# conversão de peso : lbs -> kg\n",
        "def func_converte_peso( peso_lbs ):\n",
        "  '''\n",
        "  conversão de peso : lbs -> kg\n",
        "  '''\n",
        "  peso_valor = peso_lbs.split( 'lbs', 1 )\n",
        "  return float( peso_valor[0] ) * 0.453592\n",
        "\n",
        "df_fifa['new_weight'] = df_fifa['weight'].astype('str').map( func_converte_peso )\n",
        "df_fifa.head()\n",
        "\n",
        "df_fifa['new_weight'].dtype\n",
        "\n",
        "df_fifa['new_weight'].fillna( df_fifa['new_weight'].median(), inplace=True )\n",
        "df_fifa.head()\n",
        "\n",
        "df_fifa['new_weight'].isnull().sum()\n",
        "\n",
        "# elimina as colunas originais\n",
        "df_fifa.drop(inplace=True, columns=['height','weight'])\n",
        "df_fifa.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLqIJRRAfzcV"
      },
      "source": [
        "## * Transformações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu5ubJsXg3aj"
      },
      "source": [
        "df_hw = df_fifa[['new_height','new_weight']].copy()\n",
        "df_hw.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XB3lX2wftaV"
      },
      "source": [
        "from sklearn.preprocessing import PowerTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaBqL8EQhRVR"
      },
      "source": [
        "Yeo-Johnson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip0BGjrQgj_P"
      },
      "source": [
        "# cria objeto da transformação power\n",
        "yj_trans = PowerTransformer(method = 'yeo-johnson', standardize = True)\n",
        "# aplicação da transformação, obtendo array numpy\n",
        "hw_yj = yj_trans.fit_transform(df_hw)\n",
        "# transformação do array numpy em dataframe pandas\n",
        "df_hw_yj = pd.DataFrame(hw_yj, columns = ['new_height','new_weight'])\n",
        "df_hw_yj.head()\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.hist(df_hw_yj['new_height'], color = 'blue', edgecolor = 'black', bins = int(180/5))\n",
        "\n",
        "# Adiciona títulos e labels\n",
        "plt.title('Distribuição Yeo-Johnson')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4XnKamahQph"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5fDp1Ib_Dg8"
      },
      "source": [
        "# WOE - Weight Of Evidence\n",
        "* As vantagens da transformação WOE são\n",
        "    * Lida bem com NaN's;\n",
        "    * Lida bem com outliers;\n",
        "    * A transformação é baseada no valor logarítmico das distribuições.\n",
        "    * Usando a técnica de binning apropriada, pode estabelecer uma relação monotônica (aumentar ou diminuir) entre a variável dependente e independente."
      ]
    }
  ]
}